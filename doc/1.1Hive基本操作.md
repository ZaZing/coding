[TOC]

## Hive表相关介绍



### 存储格式

Hive会为每个创建的数据库在HDFS上创建一个目录，该数据库的表会以子目录形式存储，表中的数据会以表目录下的文件形式存储。对于default数据库，默认的缺省数据库没有自己的目录，default数据库下的表默认存放在/user/hive/warehouse目录下。

**（1）textfile** 

textfile为默认格式，存储方式为行存储。数据不做压缩，磁盘开销大，数据解析开销大。 

（2）SequenceFile 

SequenceFile是Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。 

SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。 

（3）RCFile 

一种行列存储相结合的存储方式。 

（4）ORCFile 

数据按照行分块，每个块按照列存储，其中每个块都存储有一个索引。hive给出的新格式，属于RCFILE的升级版,性能有大幅度提升,而且数据可以压缩存储,压缩快 快速列存取。 

**（5）Parquet** 

Parquet也是一种行式存储，同时具有很好的压缩性能；同时可以减少大量的表扫描和反序列化的时间。

## DDL操作

### *创建表

#### 创建内部表

```
CREATE TABLE page_view(
     viewTime INT, 
     userid BIGINT,
     page_url STRING, 
     referrer_url STRING,
     ip STRING COMMENT 'IP Address of the User')
 COMMENT 'This is the page view table'
 PARTITIONED BY(dt STRING, country STRING)
 ROW FORMAT DELIMITED
   FIELDS TERMINATED BY '\001'
   COLLECTION ITEMS TERMINATED BY '\002'
   MAP KEYS TERMINATED BY '\003'
 STORED AS TEXTFILE;
```



#### 创建外部表

```
CREATE EXTERNAL TABLE page_view(
     viewTime INT, 
     userid BIGINT,
     page_url STRING, 
     referrer_url STRING,
     ip STRING COMMENT 'IP Address of the User',
     country STRING COMMENT 'country of origination')
 COMMENT 'This is the staging page view table'
 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\054'
 STORED AS TEXTFILE
 LOCATION '/user/hadoop/warehouse/page_view';
```



#### 创建分区表

```sql
create external table student_ptn(
		id int, 
		name string, 
		sex string, 
		age int,
		department string)
partitioned by (city string)
row format delimited fields terminated by ","
location "/hive/student_ptn";
```

分区

分为静态分区、动态分区，主要区别在于静态分区需要手动指定，而动态分区是基于查询参数的位置去推断分区的名称，从而建立分区。总的来说就是，静态分区的列是在编译时期通过用户传递来决定的；动态分区只有在SQL执行时才能确定。

#### 使用CTAS创建表

作用： 就是从一个查询select创建表结构，包括只复制表结构和复制表结构+数据。

```
//复制表结构
create table student_copy like student_ptn;
//复制表结构和数据
create table student_ctas as select * from student where id < 95012;

```

注意：

如果在table的前面没有加external关键字，那么复制出来的新表。无论如何都是内部表
如果在table的前面有加external关键字，那么复制出来的新表。无论如何都是外部表

### 修改表

#### 修改表名

```
alter table student rename to new_student;
```

#### 修改表分区

修改分区，一般来说，都是指修改分区的数据存储目录。

在添加分区的时候，直接指定当前分区的数据存储目录。

```
alter table student_ptn add if not exists partition(city='beijing') location '/student_ptn_beijing';
```

修改已经指定好的分区的数据存储目录

```
alter table student_ptn partition (city='beijing') set location '/student_ptn_beijing';
```

先的分区文件夹仍存在，但是在往分区添加数据时，只会添加到新的分区目录

#### 修改表列

```
//增加表字段	
alter table new_student add columns (score int);
//修改表字段定义
alter table new_student change name new_name string;
//修改字段分隔符
alter table new_student set serdeproperties('field.delim'='\t');
```

### *删除表

删除表会移除表的元数据和数据，而HDFS上的数据，如果配置了Trash，会移到.Trash/Current目录下。

删除外部表时，表中的数据不会被删除

```
DROP TABLE table_name;
DROP TABLE IF EXISTS table_name;
```

从表或者表分区删除所有行，不指定分区，将截断表中的所有分区，也可以一次指定多个分区，截断多个分区。

```
TRUNCATE TABLE table_name;
TRUNCATE TABLE table_name PARTITION (dt='20080808');
```

## DML操作

### 数据加载

#### *Load方式

Hive Load语句不会在加载数据的时候做任何转换工作，而是纯粹的把数据文件复制/移动到Hive表对应的地址。

```sql
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1,partcol2=val2 ...)]
```

- 如果命令中带有LOCAL，说明从本地文件系统加载数据，文件路径可以是相对路径，也可以是绝对路径。在这种情况下，首先将文件从本地复制到hdfs相应的位置，然后移动到hive表格中，这个时候原始数据文件是存在于hive表之下的路径下。这一点我会专门写一篇关于hive外部表的相应博文。
- 如果命令中没有LOCAL，代表我们的数据是从hdfs中读取文件，这个时候如果我们使用的是内部表，相应的hdfs的原始文件会消失，进入到相应的表格中。
- filepath 可以是一个相对路径，也可以是一个绝对路径。可以是一个文件，也可以是一个文件夹目录（这个时候文件夹下的所有文件都会被加载）
  -命令中如果带有overwirte，代表加载数据之前会清空目标表格，否则就是追加的方式。

#### *INSERT方式

##### 简单结构插入

```sql
INSERT OVERWRITE/INTO TABLE tablename [PARTITION(partcol1 = col1,partcol2 =col2…)] SELECT _statement FROM  _statement
```

动态分区插入

```sql
INSERT [OVERWRITE INTO TABLE tablename [PARTITION(partcol1 = col1,partcol2 ='${col}'…)] SELECT _statement FROM  _statement
```

##### 多重插入

多表插入指的是在同一条语句中, 把读取的同一份元数据插入到不同的表中。只需要**扫描一遍元数据**即可完成所有表的插入操作, 效率很高。

```sql
from t_x
insert into t_dest partition(p=’p1’)
select …… where …….
insert into t_dest partition(p=’p2’)
select …… where …….
```

### 数据查询

#### SELECT

提供了丰富的SQL查询方式来分析存储在Hadoop 分布式文件系统中的数据，可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询功能，可以将SQL语句转换为MapReduce任务进行运行，通过自己的SQL 去查询分析需要的内容，这套SQL 简称Hive SQL，使不熟悉mapreduce 的用户很方便的利用SQL 语言查询，汇总，分析数据。

```sql
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list [HAVING condition]]
[CLUSTER BY col_list]
[DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
[LIMIT number]
```

•使用ALL和DISTINCT选项区分对重复记录的处理。默认是ALL，表示查询所有记录。DISTINCT表示去掉重复的记录

•Where 条件

•类似我们传统SQL的where 条件

•目前支持 AND,OR ,0.9版本支持between

•IN, NOT IN

•不支持EXIST ,NOT EXIST

ORDER BY与SORT BY的不同

•ORDER BY 全局排序，只有一个Reduce任务

•SORT BY 只在本机做排序

##### 字符函数

说明：对字符进行拼接、截取、去空格

```
字符串长度函数：        length
字符串反转函数：        reverse
字符串连接函数：        concat;     concat(‘abc’,'def’,'gh’)->abcdefgh
带分隔符字符串连接函数： concat_ws;  concat_ws(',','abc','def','gh')->abc,def,gh
字符串截取函数：        substr,substring; substr('abcde',3); substring('abcde',3)
字符串截取函数：        substr,substring  substr('abcde',3,2);substring('abcde',3,2)
字符串转大写函数：      upper,ucase
字符串转小写函数：      lower,lcase
去空格函数：            trim; 去除字符串两边的空格; trim(' abc ')
左边去空格函数：        ltrim
右边去空格函数：        rtrim
正则表达式替换函数：    regexp_replace; regexp_replace(string A, string B, string C); 将字符串A中的符合java正则表达式B的部分替换为C。
							eg: regexp_replace('foobar', 'oo|ar', '')->fb
正则表达式解析函数：    regexp_extract; regexp_extract(string subject,string pattern,int index);将字符串subject按照pattern正则规则拆分返回index指定字符。
							eg: regexp_extract('foothebar', 'foo(.*?)(bar)', 0)->foothebar
								regexp_extract('foothebar', 'foo(.*?)(bar)', 1)->bar
								regexp_extract(data_field,'.*?bgStart\\=([^&]+)',1) 
URL解析函数：           parse_url;  parse_url(string urlString, string partToExtract [, string keyToExtract]);
							返回：URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO
							eg: parse_url('https://www.iteblog.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST')->facebook.com
								parse_url('https://www.iteblog.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1')->v1	
json解析函数：          get_json_object;get_json_object(string json_string, string path); 解析json的字符串json_string,返回path指定的内容
空格字符串函数：        space;  length(space(10)) -> 10
重复字符串函数：        repeat; repeat('abc',5)->abcabcabcabcabc
首字符ascii函数：       ascii;  ascii('abcde')->97
左补足函数：            lpad;   lpad('abc',10,'td')->tdtdtdtabc
右补足函数：            rpad
分割字符串函数:         split;       split('abtcdtef','t')->["ab","cd","ef"]; 返回值: array
集合查找函数:           find_in_set; find_in_set(string str, string strList);  返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。
							eg: find_in_set('ab','ef,ab,de')-> 2
								find_in_set('at','ef,ab,de')-> 0
```



##### 数学函数

```
取整函数:         round;   round(double a);        返回值: BIGINT
指定精度取整函数:  round;   round(double a, int d); 返回值: DOUBLE
向下取整函数:     floor;   返回值: BIGINT
向上取整函数:     ceil;    返回值: BIGINT
向上取整函数: 	 ceiling; 与ceil功能相同;   返回值: BIGINT   
                 银行家舍入法（1~4：舍，6~9：进，5->前位数是偶：舍，5->前位数是奇：进）；
                 bround(2.5) = 2, bround(3.5) = 4.
取随机数函数:     rand;    rand(),rand(int seed);  返回值: double
自然指数函数:     exp;     返回值: double
以10为底对数函数: log10;   返回值: double
以2为底对数函数:  log2
对数函数:         log;     log(double base, double a)
幂运算函数:       pow;     pow(double a, double p)
幂运算函数:       power;   与pow功能相同
立方根函数:       cbrt(2)
开平方函数:       sqrt
数学常数e         e()
数学常数pi        pi()
阶乘函数:         factorial(2)
二进制函数:       bin;   返回值: string  
十六进制函数:     hex;   返回值: string
反转十六进制函数: unhex
进制转换函数:     conv
绝对值函数:       abs;  返回值: double int
正取余函数:       pmod; pmod(int a, int b),pmod(double a, double b); 返回值: int double
正弦函数:         sin
反正弦函数:       asin
余弦函数:         cos
反余弦函数:       acos
正切值函数:       tan
反正切值函数:     atan
弧度值转换角度值: degrees(30)
角度值转换弧度值: radians(30)
符号函数：        sign(2); 如果a是正数则返回1.0，是负数则返回-1.
positive函数:     positive; positive(int a), positive(double a); 返回值: int double,返回a
negative函数:     negative; negative(int a), negative(double a); 返回值: int double,返回-a
```

##### 时间函数

说明：时间获取、格式化、2个时间相差、时间增加、时间减少

```
UNIX时间戳转日期函数:       from_unixtime; 返回值:string;  from_unixtime(1323308943,'yyyyMMdd')
获取当前UNIX时间戳函数: 	   unix_timestamp;返回值: bigint; unix_timestamp()
日期转UNIX时间戳函数:        unix_timestamp;返回值: bigint; unix_timestamp('2011-12-07 13:01:03')
指定格式日期转UNIX时间戳函数: unix_timestamp;返回值: bigint; unix_timestamp('20111207 13:01:03','yyyyMMdd HH:mm:ss')
日期时间转日期函数:         to_date;    返回值: string; to_date('2011-12-08 10:03:01')
日期转年函数:               year;       返回值: int;    year('2011-12-08 10:03:01')
日期转月函数:               month    
日期转天函数:               day      
日期转小时函数:             hour     
日期转分钟函数:             minute   
日期转秒函数:               second   
日期转周函数:               weekofyear; 返回值: int;    weekofyear('2019-01-01 10:03:01');   返回日期在当前的周数。
日期转天函数:               dayofmonth; 返回值: int;    weekofyear('2019-01-01 10:03:01');   返回日期在当前月的天数
日期比较函数:               datediff;   返回值: int;    datediff('2012-12-08','2012-05-09'); 返回结束日期减去开始日期的天数。
日期增加函数:               date_add;   返回值: string; date_add('2012-12-08',10);           返回开始日期startdate增加days天后的日期。
日期减少函数:               date_sub
最后一天的日期：            last_day
返回当前时间的下一个星期X所对应的日期: next_day('2015-01-14', 'TU') = 2015-01-20  以2015-01-14为开始时间，其下一个星期二所对应的日期为2015-01-20
返回当前时间属性哪个季度： quarter('2015-04-08') = 2
返回当前时间日期:         current_date
返回当前时间戳:           current_timestamp
添加月数：                add_months('2017-02-10', 2);
```

##### 窗口函数

 • 窗口函数与分析函数: 
	•用于分区排序 
	•动态Group By 
	•Top N 
	•累计计算 
	•层次查询

```sql
•排序函数：
row_number() over(partition by .. order by ..): 取partition分组order by排序后，相同值取不同序号，不存在序号跳跃
	  rank() over(partition by .. order by ..): 取partition分组order by排序后，相同值取相同序号，存在序号跳跃，不支持窗口rows从句
dense_rank() over(partition by .. order by ..): 取partition分组order by排序后，相同值取相同序号，不存在序号跳跃，不支持窗口rows从句

•聚合函数：
sum()/count()/max()/max()/min() over(partition by .. order by ..)：取partition分组order by排序后，聚合函数计算

•窗口函数：
lag(col,n,default)    over(partition by .. order by ..)：取partition分组后，查看当前行的上第n行,不指定default为null,指定为指定字段，不支持rows从句
lead(col,n,default)   over(partition by .. order by ..)：取partition分组后，查看当前行的下第n行,不指定default为null,指定为指定字段，不支持rows从句
first_value() over(partition by .. order by ..): 取partition分组order by排序后，截止到当前行|所有行，第一个值 
last_value()  over(partition by .. order by ..): 取partition分组order by排序后，截止到当前行|所有行，最后一个值  

•其他函数
cume_dist()  计算结果相对位置/总行数,不支持窗口rows从句
percent_rank() (相对位置-1)/(总行数-1),不支持窗口rows从句
percentile_disc()
percentile_count()
ratio_to_report() 计算当前行在分组中所有行数值总和所占的比例
ntile(n)      over(partition by .. order by ..): 取partition分组order by排序后，照顺序切分成n片，返回当前切片值，不支持窗口rows从句

•over从句：使用partition by与order by语句，使用一个或者多个数据类型的分区或者排序列 
注:partition内更细的划分，可使用windows子句。常见子句为：
			preceding：  往前
			following：  往后
			current row：当前行
			unbounded：  无界限（起点或终点）
			unbounded preceding：表示从前面的起点
			unbounded following：表示到后面的终点
	eg: sum(col) over(partition by .. order by .. rows between 1 preceding and current row):当前行与前一行做聚合

•窗口规范支持以下格式：窗口必须和order by 一起出现
rows between [unbounded | num] preceding and [unbounded | num] following  --当前行+往前所有行|num行+往后所有行|num行
rows between [unbounded | num] preceding and current row                  --当前行+往前所有行|num行
rows between current row                 and [unbounded | num] following  --当前行+往后所有行|num行
rows between [unbounded | num] preceding and [num] preceding
rows between [num] following and [unbounded | num] following

• order by 后面缺少窗口rows从句条件，
   窗口规范默认是 rows between unbounded preceding and current row
• order by 和窗口rows从句都缺失,    
   窗口规范默认是 rows between unbounded preceding and unbounded following
• over 从句支持排序函数、lead、lag、ntile(n) 函数，但是并不支持和窗口一起使用它们
```

##### 条件函数

```
If函数:      if; if(1=2,100,200)
非空查找函数: COALESCE;  COALESCE(null,'100','50')
非空查找函数: NVL;       NVL(expr1,expr2)
非空查找函数: NVL2;      NVL2(expr1,expr2, expr3); 第一个参数为空那么显示第二个参数的值，如果第一个参数的值不为空，则显示第三个参数的值。
非空查找函数: NULLIF;    NULLIF(exp1,expr2);      如果exp1和exp2相等则返回空(NULL)，否则返回第一个值
条件判断函数：CASE val when num1 then 'str' end; case 100 when 50 then 'tom' when 100 then 'mary' else 'tim' end
条件判断函数：CASE when val=num1 then 'str' end; case when 1=2 then 'tom' when 2=2 then 'mary' else 'tim' end
```

##### 其他常见函数

```
• explode函数: (一行转多行)将字段内复杂的数据拆分成多行, 常用来做行拆多行
				eg: explode(array): 返回多行array中对应的元素。如explode(array('A','B','C')),返回:A\B\C三行
					explode(map): 返回多行map键值对对应元素。如explode(map(1,'A',2,'B',3,'C')),返回:

• lateral view函数：(一行转多行)当表中某个字段的取值为列表或数组时，利用该函数和split、explode可以将一行拆分成多行
				eg: select deal_id ,type ,sp 
					from deal_ppt_mark_log t 
					lateral view explode(split(t.sources,','))a as sp
				行转多列可以认为多列是一个字段处理，多个转换之间lateral view不能有 "，" 分割开
				SELECT MR_Longitude,MR_Latitude,b,MAX(a) 
				FROM
				(
					select a,b,
						MR_Longitude * 360 *1.0/ 16777216 MR_Longitude,
						MR_Latitude * 90 / 8388608 MR_Latitude
					FROM etl_4g_mro_zte 
					lateral view explode(split(concat_ws(',',MR_LteScRSRP,MR_LteNcRSRP1,MR_LteNcRSRP2,),',')) r1 AS a             
					-----没有','
					lateral view explode(split(concat_ws(',',MR_LteScEarfcn,MR_LteNcEarfcn1,MR_LteNcEarfcn2),',')) f1 AS b
				)
				WHERE MR_Longitude IS NOT NULL 
				AND MR_Longitude <>''
				AND DAY=20170323;
			
• collect_set()/Collect_list()函数：(多行转一行),同组不同行合并成一行，collect_set/collect_set 去重/不去重，
                                 常常搭配concat_ws()函数使用
				eg: concat_ws(',',collect_list(cast(qty as string)))	

• str_to_map函数：str_to_map(字符串参数, 分隔符1, 分隔符2),分隔符1将文本分成K-V对，分隔符2分割每个K-V对。
				eg: str_to_map(concat_ws(',',collect_set(concat(stat_date,':',label_state))),',',':') 

• distribute by和sort by函数一起使用：ditribute by是控制map的输出在reducer是如何划分的
				eg: select mid, money, name from store distribute by mid sort by mid asc, money asc;
				    所有的mid相同的数据会被送到同一个reducer去处理，这就是因为指定了distribute by mid
					distribute by必须要写在sort by之前

• cluster by函数： cluster by的功能就是distribute by和sort by相结合
				eg: select mid, money, name from store cluster by mid sort by money;
				    注意被cluster by指定的列只能是降序，不能指定asc和desc。

• grouping sets()/cube()/rollup()高级聚合函数:
```



#### JOIN

Hive 只支持等值连接（equality joins）、外连接（outer joins）和（left semi joins）。 Hive 不支持所有非等值的连接，因为非等值连接非常难转化到 map/reduce 任务 

- LEFT，RIGHT和FULL OUTER关键字用于处理join中空记录的情况 

- LEFT SEMI JOIN 是 IN/EXISTS 子查询的一种更高效的实现 

- join 时，每次 map/reduce 任务的逻辑是这样的： reducer会缓存join序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统 

- 实践中，应该把最大的那个表写在最后

**LEFT SEMI JOIN**

LEFT SEMI JOIN 的限制是，JOIN 子句中右边的表只能在 ON 子句中设置过滤条件，在 WHERE 子句、SELECT 子句或其他地方过滤都不行

```sql
SELECT a.key, a.value 
 FROM a 
 WHERE a.key in (SELECT b.key FROM B);
可以被重写为：
 SELECT a.key, a.val 
 FROM a LEFT SEMI JOIN b on (a.key = b.key)
```

**UNION ALL**

用来合并多个select的查询结果，需要保证select中字段须一致  

```sql
select_statement UNION ALL select_statement UNION ALL select_statement ...
```

#### 集合数据结构

```
•数组类型：array， 存放相同类型的数据集合，        如：array[int] 下标访问 
•映射集合类型：map， 存放相同类型的k-v键值对集合，   如：members["father"],map_keys(members),map_values(members)
•结构体类型： struct，可存放不同类型的数据的集合，    如：struct{name:STRING,age:INT}.
```



#### 正则查询

| /     | 做为转义，即通常在"/"后面的字符不按原来意义解释，如/b/匹配字符"b"，当b前面加了反斜杆后//b/，转意为匹配一个单词的边界。对正则表达式功能字符的还原，如"*"匹配它前面元字符0次或多次，/a*/将匹配a,aa,aaa，加了"/"后，/a/*/将只匹配"a*"。 |
| ----- | ------------------------------------------------------------ |
| ^     | 匹配一个输入或一行的开头，/^a/匹配"an A"，而不匹配"An a"     |
| $     | 匹配一个输入或一行的结尾，/a$/匹配"An a"，而不匹配"an A"     |
| *     | 匹配前面元字符0次或多次，/ba*/将匹配b,ba,baa,baaa            |
| ?     | 匹配前面元字符1次或多次，/ba*/将匹配ba,baa,baaa ?	匹配前面元字符0次或1次，/ba* |
| (x)   | 匹配x保存x在名为$1...$9的变量中                              |
| x\|y  | 匹配x或y                                                     |
| {n}   | 精确匹配n次                                                  |
| {n,}  | 匹配n次以上                                                  |
| {n,m} | 匹配n-m次[xyz]	字符集(character set)，匹配这个集合中的任一一个字符(或元字符) |
| [xyz] | 匹配这个集合中的任意一个字符或元字符，[ ^xyz]表示不匹配这个集合中的任何一个字符 |
| [/b]  | 匹配一个退格符                                               |
| /b    | 匹配一个单词的边界                                           |
| /B    | 匹配一个单词的非边界                                         |
| /cX   | 这儿，X是一个控制符，//cM/匹配Ctrl-M                         |
| /d    | 匹配一个字数字符，//d/ = /[0-9]/                             |
| /D    | 匹配一个非字数字符，//D/ = /[^0-9]/                          |
| /n    | 匹配一个换行符                                               |
| /r    | 匹配一个回车符                                               |
| /s    | 匹配一个空白字符，包括/n,/r,/f,/t,/v                         |
| /S    | 匹配一个非空白字符，等于/[^/n/f/r/t/v]/                      |
| /t    | 匹配一个制表符                                               |
| /v    | 匹配一个重直制表符                                           |
| /w    | 匹配一个可以组成单词的字符(alphanumeric，这是我的意译，含数字)，包括下划线，如[/w]匹配"$5.98"中的5，等于[a-zA-Z0-9] |
| /W    | 匹配一个不可以组成单词的字符，如[/W]匹配"$5.98"中的$，等于[^a-zA-Z0-9] |

##### 常见正则

**日期相关**

日期格式：^\d{4}-\d{1,2}-\d{1,2}

一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$

一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$

**汉字**

 汉字：^[\u4e00-\u9fa5]{0,}$

**金额**

#### 

#### 从SQL到HiveQL应转变的习惯

1、Hive不支持等值连接

SQL中对两表内联可以写成：

select * from dual a,dual b where a.key = b.key;

Hive中应为

```sql
select * from dual a join dual b on a.key = b.key;
```

而不是传统的格式：

SELECT t1.a1 as c1, t2.b1 as c2FROM t1, t2

WHERE t1.a2 = t2.b2

2、分号字符

分号是SQL语句结束标记，在HiveQL中也是，但是在HiveQL中，对分号的识别没有那么智慧，例如：

```select concat(key,concat(';',key)) from dual;```

但HiveQL在解析语句时提示：

FAILED: Parse Error: line 0:-1 mismatched input '<EOF>' expecting ) in function specification

解决的办法是，使用分号的八进制的ASCII码进行转义，那么上述语句应写成：

```sql
select concat(key,concat('\073',key)) from dual;
```

**3、IS [NOT] NULL**

SQL中null代表空值, 值得警惕的是, 在HiveQL中String类型的字段若是空(empty)字符串, 即长度为0, 那么对它进行IS NULL的判断结果是False，可以 使用如下语句：

```sql
select * from table where length(filed) = 0;
```

4、Hive不支持将数据插入现有的表或分区中，仅支持覆盖重写整个表，示例如下：

```sql
INSERT OVERWRITE TABLE t1 SELECT * FROM t2;
```

5、Hive 的嵌套查询再必须给表起别名

如：          

```sql
select a.column from (select * from 表1 where dt = 20180528) a   group by   a.dt limit 100        
```

6、当有两个分区时，在使用where语句查询的时候，必须将两个分区都写出来，如：

若表的分区有两个dt日期和hr小时，我们想选择20180608这一个分区的数据，则选择条件为where dt = 20180608 and hr>= 0 

### 数据导出

#### Hive表导

```sql
INSERT OVERWRITE [LOCAL] DIRECTORY '/home/hadoop/output' ROW FORMAT DELIMITED FIELDS TERMINATED by ',' select * from testA;  
```

```
# 直接执行sql
./hive -e "select * from testA" >> /home/hadoop/output/testA.txt
# 执行sql文件
./hive -f /home/hadoop/output/sql.sql >> /home/hadoop/output/testB.txt  
```



